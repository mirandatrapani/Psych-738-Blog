---
title: "Foundations week one"
description: |
  A summary of my thoughts while reading the three articles assigned this week.
author:
  - name: Miranda Trapani 
    affiliation: CUNY Graduate Center
    affiliation_url: https://gc.cuny.edu/Home
date: 08-27-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```

<b>Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2), 81.</b>

First thoughts: I am familiar with this concept, although I don't believe I have read the original paper yet so I am looking forward to this. From what I remember it refers to how your working memory can essentially hold an average of 7 (plus or minus 2 depending on the individual!) bits of information at a time. Immediately I think of examples involving phone numbers and how handy dandy it is that most phone numbers are 7 digits long (not including area code; although no need to keep that bit in working memory!) and I also am reminded of methods of combining pieces of information to hold more in working memory, so to speak. For example if a phone number is say 333-5060, this is especially easy to remember because instead of remembering it as each individual digit you can and do think of it as the chunks "333" "50" and "60."

Thoughts while reading:<br><br>

- This article has a really interesting introduction, it reads like a personal diary entry.
- The comparison of variance and amount of information (if there is a lot of variance we don't know much about what will happen and so observations are more informative) is really interesting
- "every time the number of alternatives is increased by a factor of two, one bit of information is added." this is a cool mathematical and succinct way of describing the paragraph that made it really make sense for me
- "Fortunately, I do not have time to discuss these remarkable exceptions." I laughed.
- The Hake and Garner experiment is kind of hard to interpret; do they mean to say that it studied how accurately a person could tell the space between two points? Or remember previous positions of points? If so, how was that shown? Dots randomly placed on a white sheet? It's hard to visualize and thus harder to understand.
- Talking about how multiple variables are judged less acurately than when judging just one but skirting what this also implies about our attention being finite! Oh just kidding, this next section is talking precisely about attention. Cool!
- The observation about how the number 7 is relevant to these two very different processes but seems similar at first look, this seems like a good comparison to the field of cognition as a whole! It becomes so much more complicated the closer you look at it.
- "If the amount of information in the span of immediate memory is a constant, then the span should be short when the individual items contain a lot of information and the span should be long when the items contain little information."
- This distinction between "bits" of information and "chunks" of information (wrt absolute judgement vs immediate memory) as well as working through the integration of the two has been very helpful.
- Maybe "working memory" is distinct from "immediate memory" in that working memory involves the process of organization and the collaboration between immediate memory and absolute memory, especially to create these chunks.
- Using arithmetic with a different base number is a really clean and precise way to explain how chunking works, I just wish I understood math with different bases!
- The very ending made me think about this really awesome special Disney did in the late 50's called "Donald in Mathmagic Land" that explores the beauty of math and how math is in everything. Amazing these patterns throughout our world!

Final thoughts: This paper didn't quite cover what I expected! At first it seemed to talk more in the abstract and in some ways that made it hard to follow his exact line of thinking, as is to be expected when talking about something like information processing. The language has to be abstract in order to be applicable syntactically to all of the different kinds of information out there, I imagine! I also was interested to learn about the magic number's application on absolute judgement as previously I was only really familiar with in the context of immediate memory, as Miller calls it. I think I would like to review the first part of this paper or read other literature on absolute judgement to really get that concept solidified.





<b>Newell, A. (1973). You can't play 20 questions with nature and win: Projective comments on the papers of this symposium.</b>

First thoughts: Judging from the title of this paper I immediately thought of the trend that the more tests one runs, the more likely one is to make a Type I error, although I don't think that is what this paper will be about. 

Thoughts while reading:<br><br>

- I appreciate the more relaxed language of this paper, I expect it will be much easier to digest for that. Although maybe I just skimmed the first few pages too quickly but I have no idea what symposium this paper is referring to!
- This paper seems like a really interesting and informative snapshot of the field of Psychology and where it was at this time.
- "Thus, far from providing the rungs of a ladder by which psychology gradually climbs to clarity, this form of conceptual structure leads rather to an ever increasing pile of issues, which we weary of or become diverted from, but never really settle." In my experience with other fields, this is not a phenomenon specific to psychology! Answers to scientific questions always seem to raise more questions than they answer.
- This referenced paper, Decay of Information in Short Term Memory, sounds interesting!
- The author talks a lot about how important the method the subject takes to complete the provided tasks in these papers are and it has me thinking. I wonder how important it would be WHY the subject chose a certain method. Perhaps two subjects can go about a given taken in very different ways, but have the same reasoning behind those choices. Maybe in the same vein, two papers with vastly different explanations for a given phenomenon are just asking the wrong question? I wonder if that's where this paper is going!
-"This aspect of our current scientific style is abetted by our tendency noted at the beginning to case the results of experiments in terms of their support or refutation of various binary oppositions." I think this is a great point! A lot of results reported are framed around that single question and I wonder how much anecdotal or irrelevant information is omitted that would actually be illuminating after a meta analysis.
- As I reach the author's proposed necessary accomplishments before answering the question of how the many different phenomena studied thus far connect, his examples become harder to follow. I think it would help to paraphrase them here for myself. Having complete processing models, thus being able to explain a model for example of how something is remembered from first sensation of the stimulus to recall, makes sense. I can imagine how the understanding of these processing models would be important as well, because once you know them in their complete form you can begin to make comparisons between various processing models.
- Analyzing a complex task also makes sense in a general sense, in that this provides the opportunity to instead of hone in on a single detail and not see the trees for the forest, so to speak, you can begin to look at things up and down stream to help inform what is going on at single snapshots along the process. I'm not sure if that's what the author said, but given the example of chess I think we're on the same page.
- One program for many tasks: This is the most broad proposed solution and the one that seems the greatest undertaking, so of course its explanation is the most brief!

Final thoughts: I think this article was interesting for the way it delved deeply into one of the greater issues in the field of psychology. It's a very powerful snapshot of the field at this point in time.





<b>Schacter, D. L., Eich, J. E., & Tulving, E. (1978). Richard Semon's theory of memory. Journal of Verbal Learning and Verbal Behavior, 17(6), 721-743.</b>


First thoughts: Given the nature of the other two papers I expect this one to be a seminal analysis or think piece type that delves deep into the concept of memory as it is understood at this time and the issues and most current research in 1978, and probably implications for the future too.

Thoughts while reading:<br><br>

- This paper is less broad and more on the work of Dr. Semon who had until this point not be recognized by the community. From the statement "It is our thesis that Semon's analysis anticipated many current research problems and approaches to the study of memory in a most striking fashion, and that his work contains potentially valuable suggestions and implications for contemporary researchers," it sounds like he was ahead of his time.
- Law of engraphy: memory storage
Law of ecphory: memory retrieval  (to help keep the multiple terms straight!)
- Some really interesting and valuable history of research on memory!
- More specifically, it seems like during his time, Semon was the only one really carefully looking at recall as a process, while others siply viewed it as a problem of measurement. What exactly that means I'm not too sure.
- Further examples do make it clear that understanding of memory sidesteps the retrieval of the memory altogether and only takes into consideration that actual initial creation of the memory
- I am reminded of the phrase "finding the engram" and now know where the term engram came from in that context! This refers to our more recent attempts to, using neuroscience, look for the place in the brain where memories are physically stored. Of course emerging studies have shown that there is no one location where memories are all collected, but instead they seem to be represented more in neural pathways, or at least in multiple areas of the brain depending on the kind of memory, if I am remembering what I've heard correctly
- Immediately after writing that previous thought I reach the quote where Semon criticizes that exact line of thinking. Cool! It's interesting to hear criticisms of something that ended up being correct
- Acoluthic phase: this sounds to me similar to the time in which you can keep a stimulus in your working memory, how the "memory of a sensation" of a sound or smell or sight only lasts for so long and for each sensory modality it's different
- "Rather than positing a direct "horizontal" association between, say, Event X and Event Y, Semon suggested that X and Y form a new engram-complex by virtue of the simultaneous occurrence of the synchronous phase of Y and the fading acoluthic phase of X." This is the sentence that made it make sense for me!
- "for example, it is unclear how long the acoluthic phase lasts..." I would guess that we know this better now based on what I mentioned earlier about how long a sensation "stays in your memory" which I'm sure there is a more appropriate and accurate term for this but I can't remember it right now. Maybe long persisting visual memory traces, like is named in the next paragraph? Which means even at the time of this paper we knew better now, which I guess drives home the point of this paper that Semon's work was ahead of it's time in that it was highly applicable to psychology at least in the late 70's
- "Owing to shared components between Stimulus X and Trace Y, Y is ecphorized in the presence of X, just like in any other ecphory via partial return." To me this sounds like an argument of similarity rather than contiguity? Although I don't think I understand that difference between these two theories
- Oh later it seems more like that resemblence (and perhaps by extension the assertion or understanding that there ARE similarities) is due to contiguity; that recognition of similarity can only happen after recall. That makes sense!
- This led Semon to suggest that, "Alcoholic intoxication may, under certain circumstances, create an energetic condition whose engrams are ecphorable in the next state of intoxication, but not in the intervening state of sobriety"' This concept is something that friends and I have discussed; excited to see that there is academic thought and merit behind it!
- I am struggling a bit to wrap my head around homophony, I think perhaps it requires seeing some factor of the memory process as distinct that I'm missing out on. Maybe it's just because it's vague, as the authors say
- "REPETITION OF A STIMULUS DOES NOT STRENGTHEN AN ALREADY EXISTING ENGRAM, BUT GENERATES A NEW ENGRAM, AND THE MNEMIC EXCITATIONS RESULTING FROM ANY SUBSEQUENT ECPHORY ARE IN HOMOPHONY" This quote makes it a bit easier to understand homophony!
- "Semon did make a special effort to criticize Hoffding's (1893) theory that recognition
results from the "greater ease" of neural transmission of a second encounter with a stimulus relative to the first, noting that this theory was in direct opposition to his own "homophonous comparison" view." Considering what we now know about how in response to experiences the brain does indeed form new and stronger connections via neuroplasticity, new synaptic growth etc, this is interesting. Perhaps another reason why I don't understand homophony!

Final thoughts: I appreciate this paper as a deep analysis of the work of a psychologist especially one that I hadn't previously heard of! Especially in the opportunity to, like the previous papers, compare both the old theories that these papers recognized as old, the current field at the time of writing, and what we know now in present day.


