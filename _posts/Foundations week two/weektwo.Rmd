---
title: Foundations Week Two
description: |
  This time we learn about memory, the multiple-trace model, and many things about words!
author:
  - name: Miranda Trapani 
    affiliation: CUNY Graduate Center
    affiliation_url: https://gc.cuny.edu/Home
date: 08-27-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```

<h1>Hintzman, D. L. (1986). " Schema abstraction" in a multiple-trace memory model. Psychological review, 93(4), 411.</h1>
<br>
<p>First thoughts: Base on the title, I expect this article will be about the different methods we use to think about the information around us, by which I mean how we interpret the world around us. Stuff like exemplars, schemas,  the tools we use to see an object and determine what it is and what other processes and associations go on in the mind during that time.</p>
<p>Thoughts while reading:</p>
<ul><li>According to the intro there are essentially two views of how we become familiar with concepts. One states that every encounter of a given concept is "stored" in our brain both as the memory trace in our episodic memory but also that exposure to exemplars is stores in a unitary abstract representation of that concept which is a separate process from episodic memory. The other view is that the traces of each experience with a concept are stored and that when you think of that concept/are reminded (the retrieval process) you are recalling the aggregate of all those traces to represent the category as a whole. (Would this aggregate representation also be called a exemplar? Or a schema? Or an entirely new word?)</li>
<li>For my reference:
Primary memory: active representation of a current experience with a given phenomenon
Secondary memory: the vast pool of largely dormant memory traces for previous experiences with that given phenomenon</li>
<li>This math is pretty difficult to absorb especially given that it's mostly theory based right now since it's not being applied to a specific study or example, but essentially what I believe the authors are saying is that the strength of recall (or echo, in this paper's case) increases significantly with the increase of relevant memory traces in ones memory because if it was linear with amount of all traces, then non-relevant traces would bring to mind the "wrong" memories, so to speak.</li>
<li>I think I might have been using exemplar when talking about a concept that is actually called a prototype! </li>
<li>So this simulation, every time you re enter as a probe it's output from a previous time recalling something (that echo), you reach something closer and closer to the original category, the actual "thing" that is being rememebered with each echo. This supports the second of the views described above, in that recalling the aggregate of all traces (the aggregate of all echos previously in the current echo) leads to a more accurate representation of the given category/phenomenon/whatever it should be called.</li>
<li>This reminds me of watching graphs of sample distributions of means from a population. Watching the distribution of sample means approach the normal curve with every increase in the number of samples. (You can tell I was in statistics class right before reading this!)</li>
<li>If two categories have more similarities, categorizing a probe between the two is more difficult for the model (as well as human subjects). This makes sense intuitively, but also mechanically I believe in that as the similarities of two categories increase, so too will the traces in the echo for each probe</li>
<li>When shown higher level distortions of a prototype you are then better able to transfer items (I assume this means categorize the experimental stimuli accurately) than when you are shown lower level distortions of a prototype. This is due to having a wider category breadth</li>
<li>However, the Minerva 2 model performed opposite to that. This is explained by the fact that in previous examples with human studies that found the results above, the high level distortion participants had to practice more to get them down (required more trials to reach a performance criterion) With performance trials held constant low level distortion groups perform better, like the model. (This part seemed to be an acknowledgement of previous criticisms for this model and an explanation in support of the model's accuracy to human memory)</li>
<li>Interesting how they were able to reproduce context-dependent classification</li>
<li>"as a multiple trace model, MINERVA 2 represents an extreme form of the exemplar view." This sentence added a lot of context to the paper! They probably mentioned this in a way earlier but not in such a succinct way</li>
<li>"This is what should occur if familiar noun phrasesdirectly "look up" their meanings by activating appropriate memory traces, making it unnecessary to construct their meanings anew each time they are understood." This makes sense, I get that!</li>
<li>"Whatever the reasons were, the present simulations suggest that, perhaps with a few modifications, Semon's general approach to memory and abstraction deserves to be given a serious second look." Yay, Semon!</li></ul>

<p>Final thoughts: This paper was pretty hard to read as it involved a lot of language that was either new to me or that I am a bit rusty in, as well as a lot of computational and math theory that wasn't always very intuitive. As I went though I began to understand more as things were explained in the context of other things including analogies when given. This paper argues strongly for a theory of memory that argues that when we remember something we are essentially thinking of all the times when we saw something similar and that what decision our brain makes about what it is we are looking at or experiencing is based on what memories "echo" back due to the highest level of similarity. I wonder if this is similar to, a precursor to, or completely at odds with a resonance theory?</p>

<h1>Aujla, H., Crump, M. J., Cook, M. T., & Jamieson, R. K. (2019). The Semantic Librarian: A search engine built from vector-space models of semantics. Behavior Research Methods, 51(6), 2405-2418.</h1>

<p>First thoughts: I am interested in these two papers (this one and the next) because it seems like, judging by the titles, they are going to talk about the cognition of language! I have friends who have done/are doing linguistics research and it's really fascinating. In looking at the abstract this one isn't about linguistics in the way that I was thinking but is instead talking about a particularly intelligent search engine, I would love a search engine that can really understand what I'm searching for, enough to know what I'm looking for without whatever key word it is I can't remember that is requiring me to search in the first place! Also the potential for this kind of research to help improve search engines from the get go so they don't need to rely on years and years of user data (making the switch away from datamining giant and destroyer of internet privacy you-know-who much easier) </p>

<p>Thoughts while reading:</p>
<ul>
<li>The issue with keyword searched is exactly as I mentioned, this is exciting! I'm excited to see where this goes</li>
<li>Prediction: the paper will suggest/attempt using the findings of a latent semantic analysis to see which words are in a similar space according to this vector-space model for word meaning as the keyed entry and then bring up results that have any of those keywords in that surroundingg space. But then my question would be; would this work for phrases too or just individual words? What kind of computation (either before the search engine is for use or between the input of the search phrase and the output of the results) is necessary for this to work?</li>
<li>Oh this new semantic model in the exact next paragraph already covers one of these issues; grammar and word order considerations definitely seems like it would be able to consider phrases or maybe even a whole sentence?</li>
<li>Makes sense to start this with a smaller subset of things to comb through to find results; just the experimental psych publications I mean</li>
<li>I wish there was a bit more background knowledge, especially with regards to some of the vocabulary used here like corpus and what is means to "scrape" data (although I can imagine that's along the lines of just "we had a program go through these documents and save these features of each one to a repository!" or something along those lines, but it would be nice to know for sure!)</li>
<li>It makes more sense for the window around the target word to be sensitive of whether words are in the same sentence or not</li>
<li>This kind of model would have to be adjusted to the conventions of every language! For example in latin, although it's a dead language, the word order doesn't actually change the meaning of a sentence and while there are some conventions, works of poetry (as most of the latin literature the we study/read/consume today are) in particular break contradictions or expectations routinely in order to improve the prose (in particular to make sure the work stays in meter)</li>
<li>"In our experience, author search can help users discover scientists who examine ideas similar to those that the users themselves are interested in, but of whom they may not previously have been aware because of differences in language use." This is so awesome! I would definitely benefit from this!</li>
<li>Earlier in the article it was mentioned briefly that the search enginge didn't really work when the phrase was replaced by synonyms, I'm a bit curious to hear more about that. What does this mean for the search engine's potential applicability to papers on similar topics but using different language? Is it possible to, basically, give the search engine a thesaurus? Maybe it's just a matter of feeding that information to it?</li></ul>

<p>Final thoughts: This was a really interesting paper and I got especially excited reading the final thoughts in the general discussion about the different ways that this kind of search enginge could be applied. Very interdisciplinary which always leads to cooler things, if you ask me! At first this paper was a bit difficult to get through, especially the methods and understanding how exactly this engine works (more specifically the BEAGLE model it uses) but the discussion was much easier to digest and very interesting.</p>


<h1>Crump, M. J., Lai, W., & Brosowsky, N. P. (2019). Instance theory predicts information theory: Episodic uncertainty as a determinant of keystroke dynamics. Canadian Journal of Experimental Psychology/Revue canadienne de psychologie exp√©rimentale, 73(4), 203.</h1>


<p>First thought: Judging from the abstract and the title, this paper is about how you can tell based on how someone is typing whether they know where the next letter they want is, essentially. I think! The abstract says that interkeystroke interval (the pauses between each letter) is usually related to not knowing where a letter is rather than your brain deciding which letter or word is going to come next. That's my guess on what this is going to be about! I imagine it will be a study timing the brief pauses between each key stroke a person makes in a variety of different contexts and comparing.</p>

<p>Thoughts while reading:</p>
<ul>
<li>IKSI: inter keystroke intervals (for the letters within a single world)</li>
<li>Shannon's H: is 0 when an event is completely predictable (occurs 100% of the time) and gets higher as entropy/unpredictability increases (but is H a maximum value when that event occurs 0% of the time? Or rather, when there is an infinite number of possibilities such that any given one has essentially a 0% possibility of occurring?)</li>
<li>In this case H = 4.7 when all letters are equally probable</li>
<li>"We included IKSIs only for keystrokes involving a lower case letter, and only for correct keystrokes that were preceded by a correct keystroke. Outlier IKSIs were removed for each subject, on a cell-by-cell basis, using the Van Selst and Jolicoeur (1994) nonrecursive moving criterion procedure, which eliminated approximately 3% of IKSIs from further analysis." From this I can gather that typos happened at MOST 3% of the time which even by that estimate is not a lot of typos at all! I'm impressed by how well these people can type! </li>
<li>So essentially when a word is especially long, there is a pause between the first letter and all subsequent letters. I would imagine this pause it pretty quick, perhaps something you don't quite recognize yourself yourself as you are typing, because I am paying attention to my own typing speed as I type these words and I'm not sure if I can actually notice it or if I'm experiencing confirmation bias and it's indiscernible to me.</li>
<li>Also! The authors claim that the midword slowing effect increases with word length but plateus at about 8 but they only went up to 9! I wonder if looking at longer and longer words would show the plateau or not</li>
<li>Shannon's H is higher for the first position letter and for the center letter position! The same positions that we experience these slowing effects with! Cool! (But correlation does not imply causation, of course)</li>
<li>But looking at the probability of a two letter group (a bigram) allowed for a stronger relationship between H and IKSI than looking at each literally individually (as a unigram)</li>
<li>I got pretty confused when it came to the example of instance theory modelling learning; I think what it's saying is that learning works by memory in that memory-based responses to a stimuli become a race for whichever trace is echoed first, so the more you do this (practice/repeat/study?) the more traces you have, so the higher probability that one will be very fast? However if the retrieval times vary across traces in a normal curve distribution, this doesnt necessarily make sense to me. Wouldn't more and more traces just bring you closer and closer to the mean? And less practiced stimuli just experience a less normal but not necessarily consistently fast or slow range of responses? Am I completely misunderstanding this part?</li>
<li>Ok so to make sure I am understanding right I am going to paraphrase here again: After the study with human participants they defined the probabilities of a given letter or string of letters (the letter uncertainty) by a few aspects of the word, and did a similar thing to the previous studies where they fed a model the characteristics of these variables, and then ran samples of the same test but with the simulation. (Forgive the broad and perhaps inaccurate language)</li>
<li>This was done to determine whether the model operating udner a given theory (there the instance theory of automatization) would accurately reproduce the results seen with humans. Which it seems to have!</li>
<li>"The extent to which our results generalise to the case of composition typing is unclear, and we
expect that first-letter slowing effects would be greatly increased during composition typing, reflecting the increased demands on word and sentence construction compared to copy-typing." I think that this definitely makes sense and also makes me realize that my own effort earlier in comapring my typing with the findings for the copy typing task was a bit misguided; not the same thing anyway!</li>

<p>Final thoughts: This paper was a bit harder to digest than the previous one as it relied a lot on the theory language that I tend to struggle with when it comes to cognition papers, but I did feel that the previous readings for this week and last helped me and I was able to make connections or understand references I may not have before! For this paper my biggest struggle was following the math, I think. </p>


